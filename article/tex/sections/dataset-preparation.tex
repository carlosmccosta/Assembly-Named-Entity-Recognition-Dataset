\section{Dataset preparation}\label{sec:dataset-preparation}

Automatic extraction of text from \gls{pdf} files with multi-column text, tables and large number of images and diagrams is a challenging task for any \gls{nlp} system. As such, the dataset preparation included the automatic extraction of text from the \gls{pdf} files, followed by a manual cleaning and inspection phase in which all the text that was not related to assembly operations was removed. To speedup this process and ensure proper text cleaning across the entire dataset, it was applied a set of regular expressions in order to remove page headers and footers and correct formating issues related with the text extraction. After this preprocessing stage, the dataset was proofread to correct spelling errors. Later on the lists / tables with the information about the required assembly parts / tools was moved into validation files in order to allow the evaluation of a given \gls{ner} system.

Given that some \gls{nlp} toolkits such as the \gls{srilm} \cite{Stolcke2002} expect tokenized text when building N-gram models, a set of regular expressions was applied to the dataset in order to separate the words from the punctuation. Later on, the cleaned text extracted from the \glspl{pdf} was merged into training (75\%) and testing (25\%) files according to its respective category, namely the alternators, engines, gearboxes and the entire dataset. These merged files have two versions, one with the original cleaned text and another version with the tokenized text.
